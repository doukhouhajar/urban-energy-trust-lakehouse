# Urban Energy Data Lakehouse with Data Quality & Governance
## London Pilot â€” Smart City Decision-Making

A production-grade data lakehouse architecture implementing Bronze/Silver/Gold layers with comprehensive data quality validation, governance, and ML-powered quality risk prediction for smart meter electricity consumption data.

---

## ðŸ—ï¸ Architecture Overview

### Lakehouse Layers

1. **Bronze Layer** (`lakehouse/bronze/`)
   - Raw data ingestion with minimal transformation
   - Preservation of original data with ingestion metadata
   - Supports both batch and streaming ingestion

2. **Silver Layer** (`lakehouse/silver/`)
   - Cleaned and validated data
   - Standardized schemas
   - Deduplication and temporal coherence fixes
   - Missing value imputation/repair

3. **Gold Layer** (`lakehouse/gold/`)
   - Analytics-ready trusted tables
   - `quality_scores` - Dataset quality scores (0-100) per partition
   - `quality_incidents` - Detailed quality incident logs
   - `quality_risk_predictions` - ML predictions for future quality risk

### Tech Stack

- **PySpark** - Distributed data processing
- **Delta Lake** - ACID transactions, time travel, schema evolution
- **Great Expectations** - Data quality validation framework
- **Apache Sedona** - Geospatial processing (GADM, OSM)
- **LightGBM** - Quality risk prediction ML model
- **Spark Structured Streaming** - Real-time data ingestion
- **Hive Metastore** - Table metadata management

---

## ðŸ“‹ Prerequisites

- Python 3.9+
- Java 8 or 11
- Docker & Docker Compose (for Spark cluster)
- 16GB+ RAM recommended

---

## ðŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Start Infrastructure (Docker Compose)

```bash
docker-compose up -d
```

This starts:
- Spark Master (localhost:8080)
- Spark Worker
- Hive Metastore (PostgreSQL backend)
- Jupyter (optional, localhost:8888)

### 3. Run Batch Pipeline

```bash
# Full end-to-end batch pipeline
python -m src.pipelines.batch_pipeline

# Or using Make
make batch-run
```

### 4. Run Streaming Pipeline

```bash
# Start streaming ingestion (simulates new data arrival)
python -m src.pipelines.streaming_pipeline

# Or using Make
make streaming-run
```

### 5. Train Quality Risk ML Model

```bash
python -m src.ml.train_quality_model

# Or using Make
make ml-train
```

---

## ðŸ“ Project Structure

```
urban-energy-trust-lakehouse/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml                 # Main configuration
â”‚   â”œâ”€â”€ great_expectations/         # GE expectations suites
â”‚   â””â”€â”€ schemas/                    # JSON schema definitions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ batch_ingestion.py      # Batch data ingestion
â”‚   â”‚   â””â”€â”€ streaming_ingestion.py  # Streaming data ingestion
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ great_expectations_suite.py  # GE validations
â”‚   â”‚   â”œâ”€â”€ custom_checks.py        # Custom Spark quality checks
â”‚   â”‚   â””â”€â”€ quality_scoring.py      # Quality score computation
â”‚   â”œâ”€â”€ governance/
â”‚   â”‚   â”œâ”€â”€ schema_registry.py      # Schema evolution handling
â”‚   â”‚   â”œâ”€â”€ audit_log.py            # Pipeline audit logging
â”‚   â”‚   â””â”€â”€ versioning.py           # Delta time travel utilities
â”‚   â”œâ”€â”€ geospatial/
â”‚   â”‚   â”œâ”€â”€ gadm_ingestion.py       # GADM admin boundaries
â”‚   â”‚   â”œâ”€â”€ osm_ingestion.py        # OSM building extraction
â”‚   â”‚   â””â”€â”€ spatial_operations.py   # Spatial joins & aggregations
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ quality_features.py     # Feature engineering for ML
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ train_quality_model.py  # Model training
â”‚   â”‚   â”œâ”€â”€ predict_quality_risk.py # Inference script
â”‚   â”‚   â””â”€â”€ model_utils.py          # Model utilities
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ batch_pipeline.py       # End-to-end batch pipeline
â”‚       â””â”€â”€ streaming_pipeline.py   # End-to-end streaming pipeline
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb        # Data exploration
â”‚   â”œâ”€â”€ 02_quality_analysis.ipynb   # Quality analysis
â”‚   â””â”€â”€ 03_ml_evaluation.ipynb      # ML model evaluation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_quality.py
â”‚   â””â”€â”€ test_governance.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw source data
â”‚   â””â”€â”€ lakehouse/                  # Delta Lake tables
â”‚       â”œâ”€â”€ bronze/
â”‚       â”œâ”€â”€ silver/
â”‚       â””â”€â”€ gold/
â”œâ”€â”€ docker-compose.yml              # Spark cluster setup
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ Makefile                        # Convenience commands
â””â”€â”€ README.md

```

---

## ðŸ” Data Quality Rules

### Completeness
- Missing rate per household per day/week
- Threshold: Alert if >10% missing per day, >25% per week

### Temporal Coherence
- Half-hourly cadence must be consistent (30-minute intervals)
- Detect gaps, duplicates, out-of-order records
- Alert if >5% temporal anomalies

### Business Rules
- Negative consumption invalid (hard constraint)
- Extreme spikes: z-score >5 or absolute >50 kWh/hh (contextual)
- Cross-source consistency: consumption should correlate with weather extremes

### Schema Validation
- Type checks (numeric, datetime, categorical)
- Allowed ranges for energy values (0-100 kWh/hh)
- Categorical enums for tariffs (Std/ToU), ACORN groups

### Quality Scoring Formula

```
Quality Score (0-100) = 
  40 * (1 - completeness_error_rate) +
  25 * (1 - temporal_coherence_error_rate) +
  20 * (1 - business_rule_violation_rate) +
  15 * schema_validity_score

Where:
  - completeness_error_rate = missing_records / expected_records
  - temporal_coherence_error_rate = anomalies / total_records
  - business_rule_violation_rate = violations / total_records
  - schema_validity_score = valid_records / total_records
```

---

## ðŸ“Š Example Queries

### Top Areas by Low Quality Score

```sql
SELECT 
    acorn_grouped,
    DATE_TRUNC('day', score_date) as day,
    AVG(quality_score) as avg_score,
    COUNT(*) as household_count
FROM gold.quality_scores
WHERE score_date >= CURRENT_DATE - INTERVAL 7 DAYS
GROUP BY acorn_grouped, DATE_TRUNC('day', score_date)
ORDER BY avg_score ASC
LIMIT 20;
```

### Incidents by Type Over Time

```sql
SELECT 
    DATE_TRUNC('day', incident_timestamp) as day,
    rule_name,
    severity,
    COUNT(*) as incident_count,
    COUNT(DISTINCT entity_id) as affected_households
FROM gold.quality_incidents
WHERE incident_timestamp >= CURRENT_DATE - INTERVAL 30 DAYS
GROUP BY day, rule_name, severity
ORDER BY day DESC, incident_count DESC;
```

### Quality Risk Predictions

```sql
SELECT 
    prediction_date,
    entity_id,
    risk_score,
    risk_category,
    top_features,
    model_version
FROM gold.quality_risk_predictions
WHERE prediction_date = CURRENT_DATE + INTERVAL 1 DAY
ORDER BY risk_score DESC
LIMIT 100;
```

### Building Aggregation by Admin Area

```sql
SELECT 
    g3.NAME_3 as admin_area,
    COUNT(b.id) as building_count,
    SUM(ST_Area(b.geometry)) as total_area_m2
FROM gold.osm_buildings b
JOIN gold.gadm_level3 g3 
    ON ST_Within(ST_Transform(b.geometry, 4326), g3.geom)
WHERE g3.NAME_0 = 'United Kingdom'
GROUP BY g3.NAME_3
ORDER BY building_count DESC
LIMIT 50;
```

---

## ðŸ”„ Time Travel & Versioning

Delta Lake provides time travel capabilities. Access historical versions:

```python
from src.governance.versioning import get_table_version

# View table at a specific version
df = spark.read.format("delta").option("versionAsOf", 5).load("lakehouse/gold/consumption")

# View table at a specific timestamp
df = spark.read.format("delta").option("timestampAsOf", "2024-01-01").load("lakehouse/gold/consumption")

# Get version history
history = spark.sql("DESCRIBE HISTORY delta.`lakehouse/gold/consumption`")
history.show()
```

---

## ðŸ“ˆ ML Model: Quality Risk Prediction

### Problem Definition
Predict whether a future time window (e.g., next day) will have low quality (quality score < threshold OR high incident count).

### Features
- Historical missing rate (7-day, 30-day rolling)
- Incident counts by type (temporal, completeness, business)
- Volatility metrics (consumption std dev, z-score)
- Weather statistics (temperature, humidity extremes)
- Household segment (ACORN group encoding)
- Time-of-year signals (month, day_of_week, is_holiday)

### Model Outputs
- `quality_risk_predictions` Delta table
- Feature importance visualization
- Model metrics (AUC, F1, calibration curve)
- Explainability fields (SHAP values or feature contributions)

---

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test suite
pytest tests/test_quality.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ðŸ“ Configuration

Edit `config/config.yaml` to customize:
- Data paths
- Table names
- Quality thresholds
- ML model parameters
- Spark configuration

---

## ðŸ³ Docker Commands

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f spark-master
docker-compose logs -f spark-worker

# Stop services
docker-compose down

# Rebuild images
docker-compose build --no-cache
```

---

## ðŸ“š Additional Resources

- **Delta Lake Documentation**: https://delta.io
- **Great Expectations**: https://greatexpectations.io
- **Apache Sedona**: https://sedona.apache.org
- **Spark Structured Streaming**: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html

---

## ðŸŽ¯ Acceptance Criteria Status

- âœ… End-to-end batch pipeline with Bronze/Silver/Gold layers
- âœ… Streaming pipeline with Structured Streaming
- âœ… Great Expectations + custom quality checks
- âœ… Quality scoring and incident logging
- âœ… Geospatial ingestion (GADM + OSM) with spatial operations
- âœ… ML model for quality risk prediction
- âœ… Schema evolution and audit logging
- âœ… Time travel/versioning support
- âœ… Comprehensive documentation and example queries

---

## ðŸ‘¥ Authors

Urban Energy Trust - London Pilot Team

---

## ðŸ“„ License

MIT License
